adjust
fusions --scale8
qtune --step * scheme=float float_type=bfloat16
nodeoption 0 ALLOCATE 1

# RAM/FLASH Settings
set l3_ram_device $(MODEL_L3_RAM)
set l3_flash_device $(MODEL_L3_FLASH)
set graph_const_exec_from_flash true

set privileged_l3_flash_device $(MODEL_SEC_L3_FLASH)
set privileged_l3_flash_size 1900000

# Perf and Execution Monitoring
set graph_produce_node_names true
set graph_produce_operinfos true
set graph_monitor_cycles true
#set graph_trace_exec true

# Debug Checksum/Entire Tensor
#set graph_checksum 1
#set graph_dump_tensor 0

save_state
